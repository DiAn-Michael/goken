

ALBERTについて

０．概要
自然言語表現を事前トレーニングするときにモデルを大きくしたら
ダウンストリーム（サーバ側からクライアント側へ向かうデータの流れのこと）
タスクのパフォーマンスが向上する。

ただしある時点で、GPU/TPUメモリ制限と長いトレーニング時間のためモデルの増加が難しくなる。

これらの問題に対処するためにメモリ消費を減らし
BERTのトレーニング速度を向上させる2つのパラメータ削減手法を紹介する。

こっから提案する内容のほうが元のBERTに比べてはるかに優れたモデル。

コードと事前学習済みモデルがgithubに挙げられている。
https://github.com/google-research/ALBERT.


１．introduction
実際のアプリケーションでは大きなモデルを事前にトレーニングしそれらをより
小さなモデルに蒸留することを一般的に行っている。
現在の最先端のモデルは数億、数十億ものパラメータであることが多いので、
モデルをスケーリングしようとすると簡単に使用可能なハードウェアのメモリ制限に達する。
既存の解決策としてモデルの並列化とメモリ管理が挙げられる。

従来のBERTよりもパラメータを大幅に少なくしたLite BERT(ALBERT)で前述の問題全て解決する。

☆ALBERTについて☆
事前トレーニング済みモデルのスケーリング（正規化や標準化）における
主要な障害を取り除く2つのパラメータ削減手法が組み込まれている。
1つ目
大きな語彙の埋め込み行列を2つの小さな行列に分解することにより、
隠れたレイヤーのサイズを語彙の埋め込みのサイズから分離します。
この分離により、語彙の埋め込みのパラメーターサイズを大幅に増やすことなく、
隠しサイズを簡単に拡大できます。

2つ目
レイヤー間パラメーターの共有でネットワークの深さと共にパラメーターが大きくなるのを防ぐ。

この2つの手法でパラメータ数を大幅に減らします。
BERT-leargeと同様のALBERT構成では、パラメータが18倍少なく、
トレーニングを1.7倍はやめることが出来る。

ALBERTのパフォーマンスをさらに改善するために、
文順予測（SOP）のself-supervised loss も導入。

３．ALBERTの要素
3-1.モデルアーキテクチャの選択
GELU nonlinearitiesでトランスフォーマーエンコーダーを使用するという点でBERTに似ている。
BERT表記規則に従い、ボキャブラリーの埋め込みサイズをE、
エンコーダーレイヤーの数をL、隠しサイズをHと表記します。 
フィードフォワード/フィルターサイズを4Hに、アテンションヘッドの数をH / 64に設定しました。

因数分解埋め込みパラメーター化。
埋め込みパラメータの因数分解を使用して2つの小さな行列に分解します。
サイズHの場合、最初にそれらをサイズEの低次元の埋め込みスペースに投影し、
次にそれを非表示スペースに投影します。
この分解を使用して、埋め込みパラメーターをO（V×H）からO（V×E + E×H）に減らします。
全ての単語の部分に同じEを使用する。これによりドキュメント全体にはるかに均等に分散される。

レイヤー間パラメーターの共有
レイヤー間ですべてのパラメーターを共有している。
これにより、レイヤーからレイヤーへの移行はBERTYよりもはるかにスムーズになる。
この結果より、重みの共有がネットワークパラメータの安定化に影響を与えることを示す。

文間のコヒーレンス損失
NSPでなく、文順予測（SOP）損失を使用。

3-2.モデルセットアップ
BERTよりもALBERTのパラメータ数がかなり減ったよっていう話。

4.実験結果
まあいいかと思い割愛。

